# OptiTest.jl

[![Build Status](https://github.com/sandyspiers/OptiTest.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/sandyspiers/OptiTest.jl/actions/workflows/CI.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/sandyspiers/OptiTest.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/sandyspiers/OptiTest.jl)
[![Code Style: Blue](https://img.shields.io/badge/code%20style-blue-4495d1.svg)](https://github.com/invenia/BlueStyle)

A semi-automated toolkit to run large-scale, distributed numerical experiments on your optimisation functions and to analyse the results.
Includes several experiment-level helper functions and setups used to run your tests.
Experiments can be defined using an easy to read and reused dictionary format.
This allows experiments to be easily run in your Julia REPL, or run as a script on a server.
The results are saved in a standardized format, and several analysis tools are included.
Style guides can be provided to produce semi-automated performance metrics with a standard formatting (such as colouring).

## Usage

An _experiment_ defines a set of _tests_ which should be conducted, with each test returning a standardized _result_ object.
Each experiment is defined by 3 top level attributes:

- `setup`: this defines all top-level parameters.
   This includes things such as name, directory, log, save and setup file, number of parallel experiments, etc.
   It should also define the generic solver routine.
- `instance`: contains two subfields: `generator`, which should be a callable function, and `parameters`, a dictionary
   defining the parameters used to generate an instance.
   We can then call `ins = generator(parameters...)` to instantiate this particular instance.
   For example, with a random generator, this includes parameters such as size, seed, etc.
   For file based generator, could include the instance file list, (potentially generated by code, i.e., `glob("dir")`).
- `solver`: the solver settings and parameters used to actually solve the problem

It should be then possible to call the generic solver procedure
with the `instance` and `solver` sub-dictionaries to run a single test (`setup.generic_solver(instance, solver)`).
This function should return a standardized dictionary of recordable results.
It is up to the user to define this generic solver. An example is shown below.

```julia
generic_solve(test::Test) = generic_solve(test.setup, test.instance, test.solver)
function generic_solve(setup::Setup, instance::Instance, solver::AbstractDict)
    # generate instance
    ins = instance.generator(instance.parameters...)
    # solve the instance using solver solver settings
    result = solve!(ins, solver)
    return result
end
```

### Repeatable Parameters

The key part of this package is the ability to product over certain parameters to create
generators for tests.
For example, `instance.parameters` is a dictionary field.
To product over a dictionary, suffix the desired key by "!" or "::iterate".
This will then create generator where each experiment has one of the elements listed under the element defined.
The same can be achieved if `solver` is defined as a dictionary.
There are two additional cases of repeatable parameters; that is if `instance.generator` or `solver` are given
as a vector.
In such case, we do the same repetition at the top level.
Here are some examples:

```julia
```

### Parallel Tests

It is the users responsibility to provide some distributed workers to conduct an experiment.
This may seem annoying, but it allows for warmstarting, precompling, image building (maybe?) and remote workers.
